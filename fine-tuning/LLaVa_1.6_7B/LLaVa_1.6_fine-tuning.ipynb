{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib -q -U\n",
    "!pip install datasets -q -U\n",
    "!pip install -q bitsandbytes sentencepiece  accelerate loralib\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install hf_transfer -q -U\n",
    "!pip install pickleshare -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows for faster downloads\n",
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klone nach 'LLaVA'...\n",
      "remote: Enumerating objects: 2297, done.\u001b[K\n",
      "remote: Total 2297 (delta 0), reused 0 (delta 0), pack-reused 2297 (from 1)\u001b[K\n",
      "Empfange Objekte: 100% (2297/2297), 13.71 MiB | 12.02 MiB/s, fertig.\n",
      "Löse Unterschiede auf: 100% (1405/1405), fertig.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('LLaVA'):\n",
    "    !git clone https://github.com/haotian-liu/LLaVA.git\n",
    "else:\n",
    "    print(\"LLaVA already exists. Skipping clone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File modified successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the path to the builder.py file\n",
    "file_path = '/Users/fabian.fuerst/Documents/GitHub/Fine-tuned-LLaVA-Vision-and-Language/fine-tuning/LLaVa_1.6_7B/LLaVA/llava/model/builder.py'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Regular expression to find the block between 'vision_tower = model.get_vision_tower()' and 'vision_tower.image_processor'\n",
    "pattern_block = (\n",
    "    r'(vision_tower = model.get_vision_tower\\(\\)\\n)'\n",
    "    r'.*?' # Non-greedy match for any characters\n",
    "    r'(image_processor = vision_tower.image_processor)'\n",
    ")\n",
    "\n",
    "replacement_block = (\n",
    "    r'\\1' # Keep the first line unchanged\n",
    "    '       if not vision_tower.is_loaded:\\n'\n",
    "    '           print(\\'vision_tower is not loades so loading now\\')\\n'\n",
    "    '           vision_tower.load_model(device_map=device_map)\\n'\n",
    "    '           vision_tower.to(device=device_map, dtype=torch.bfloat16)\\n'\n",
    "    '       else:\\n'\n",
    "    '           print(\\'vision_tower is already loaded\\')\\n'\n",
    "    r'      \\2' # Keep the last line unchanged\n",
    ")\n",
    "\n",
    "# Replace the specific block\n",
    "content = re.sub(pattern_block, replacement_block, content, flags=re.DOTALL)\n",
    "\n",
    "# Write the content back to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print('File modified successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No modification needed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the path to the builder.py file\n",
    "file_path = '/Users/fabian.fuerst/Documents/GitHub/Fine-tuned-LLaVA-Vision-and-Language/fine-tuning/LLaVa_1.6_7B/LLaVA/llava/model/builder.py'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Regular expression to find 'float16' and replace it with 'bfloat16'\n",
    "pattern = r'(?<!b)float16'\n",
    "\n",
    "# CHeck if there are any matches\n",
    "if re.search(pattern, content):\n",
    "    # Replace all matches\n",
    "    modified_content = re.sub(pattern, 'bfloat16', content)\n",
    "\n",
    "    # Write the content back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "    print('File modified successfully.')\n",
    "else:\n",
    "    print('No modification needed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fabian.fuerst/Documents/GitHub/Fine-tuned-LLaVA-Vision-and-Language/fine-tuning/LLaVa_1.6_7B/LLaVA\n"
     ]
    }
   ],
   "source": [
    "%cd LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: typer 0.15.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Takes quite a while to run\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"installl\" - maybe you meant \"install\"\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[22 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/jw/9kbqqtt11wj27s6z8wmgk_yw0000gp/T/pip-install-2jbcb61h/deepspeed_b0e5288316eb4ed2be57c7e5651e6737/setup.py\", line 38, in <module>\n",
      "  \u001b[31m   \u001b[0m     from op_builder.all_ops import ALL_OPS\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/jw/9kbqqtt11wj27s6z8wmgk_yw0000gp/T/pip-install-2jbcb61h/deepspeed_b0e5288316eb4ed2be57c7e5651e6737/op_builder/all_ops.py\", line 29, in <module>\n",
      "  \u001b[31m   \u001b[0m     builder = get_accelerator().create_op_builder(member_name)\n",
      "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/jw/9kbqqtt11wj27s6z8wmgk_yw0000gp/T/pip-install-2jbcb61h/deepspeed_b0e5288316eb4ed2be57c7e5651e6737/accelerator/mps_accelerator.py\", line 234, in create_op_builder\n",
      "  \u001b[31m   \u001b[0m     builder_class = self.get_op_builder(op_name)\n",
      "  \u001b[31m   \u001b[0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/jw/9kbqqtt11wj27s6z8wmgk_yw0000gp/T/pip-install-2jbcb61h/deepspeed_b0e5288316eb4ed2be57c7e5651e6737/accelerator/mps_accelerator.py\", line 241, in get_op_builder\n",
      "  \u001b[31m   \u001b[0m     from deepspeed.ops.op_builder.cpu import NotImplementedBuilder\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/jw/9kbqqtt11wj27s6z8wmgk_yw0000gp/T/pip-install-2jbcb61h/deepspeed_b0e5288316eb4ed2be57c7e5651e6737/deepspeed/__init__.py\", line 21, in <module>\n",
      "  \u001b[31m   \u001b[0m     from . import ops\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/jw/9kbqqtt11wj27s6z8wmgk_yw0000gp/T/pip-install-2jbcb61h/deepspeed_b0e5288316eb4ed2be57c7e5651e6737/deepspeed/ops/__init__.py\", line 6, in <module>\n",
      "  \u001b[31m   \u001b[0m     from . import adam\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/jw/9kbqqtt11wj27s6z8wmgk_yw0000gp/T/pip-install-2jbcb61h/deepspeed_b0e5288316eb4ed2be57c7e5651e6737/deepspeed/ops/adam/__init__.py\", line 6, in <module>\n",
      "  \u001b[31m   \u001b[0m     from .cpu_adam import DeepSpeedCPUAdam\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/jw/9kbqqtt11wj27s6z8wmgk_yw0000gp/T/pip-install-2jbcb61h/deepspeed_b0e5288316eb4ed2be57c7e5651e6737/deepspeed/ops/adam/cpu_adam.py\", line 7, in <module>\n",
      "  \u001b[31m   \u001b[0m     from cpuinfo import get_cpu_info\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'cpuinfo'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[22 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Schwerwiegend: Kein Git-Repository (oder irgendeines der Elternverzeichnisse): .git\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.1.2\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m /private/var/folders/jw/9kbqqtt11wj27s6z8wmgk_yw0000gp/T/pip-install-e3mpb7rf/flash-attn_f31758835e37460888dde968f1bdb020/setup.py:106: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/jw/9kbqqtt11wj27s6z8wmgk_yw0000gp/T/pip-install-e3mpb7rf/flash-attn_f31758835e37460888dde968f1bdb020/setup.py\", line 190, in <module>\n",
      "  \u001b[31m   \u001b[0m     CUDAExtension(\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/fabian.fuerst/Documents/GitHub/Fine-tuned-LLaVA-Vision-and-Language/.conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1076, in CUDAExtension\n",
      "  \u001b[31m   \u001b[0m     library_dirs += library_paths(cuda=True)\n",
      "  \u001b[31m   \u001b[0m                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/fabian.fuerst/Documents/GitHub/Fine-tuned-LLaVA-Vision-and-Language/.conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1203, in library_paths\n",
      "  \u001b[31m   \u001b[0m     if (not os.path.exists(_join_cuda_home(lib_dir)) and\n",
      "  \u001b[31m   \u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/fabian.fuerst/Documents/GitHub/Fine-tuned-LLaVA-Vision-and-Language/.conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2416, in _join_cuda_home\n",
      "  \u001b[31m   \u001b[0m     raise OSError('CUDA_HOME environment variable is not set. '\n",
      "  \u001b[31m   \u001b[0m OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf -q -U\n",
    "!pip installl --upgrade Pillow -q\n",
    "!pip install -e '.[train]' -q\n",
    "!pip install flash-attn --no-build-isolation -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabian.fuerst/Documents/GitHub/Fine-tuned-LLaVA-Vision-and-Language/.conda/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_path = 'liuhaotian/llava-v1.6-mistral-7b'\n",
    "\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name,\n",
    "    cache_dir='',\n",
    "    use_flash_attn=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
